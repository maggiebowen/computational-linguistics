{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzpLKqEsVyv8",
        "outputId": "1fdddf22-e429-44d0-98e3-705a7247fda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.10/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.12.14)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2025.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "        level1  label\n",
            "0  NOT ABUSIVE      0\n",
            "1  NOT ABUSIVE      0\n",
            "2      ABUSIVE      1\n",
            "3  NOT ABUSIVE      0\n",
            "4  NOT ABUSIVE      0\n",
            "Training Texts Sample: [\"why is ankara and media in turkey dictating our presence in afghanistan? the tweets go back many years. it's the brotherhood and jihadists that are running ðŸ‡ºðŸ‡¸ the need to get rid of ðŸ‡®ðŸ‡± they hate jews always have!\", '[USER] newsflash, asshole, muslims invented shit like making jews wear yellow stars to distinguish us and make it easier to identify and oppress within the first century that they existed', 'people of iran will turn the incoming presidential circus into a referendum on the legitimacy of the islamic republic. polls suggest about 80% say no. [USER] [USER] who are you negotiating with?! [USER] [USER] [USER] [USER] [URL]']\n",
            "Training Labels Sample: [1, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install googletrans==3.1.0a0\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from googletrans import Translator\n",
        "\n",
        "# Load Dataset\n",
        "def load_english_data(filepath):\n",
        "    data = pd.read_csv(filepath, sep=\"\\t\")\n",
        "\n",
        "    # Clean labels by stripping leading/trailing spaces (no internal space changes)\n",
        "    data['level1'] = data['level1'].str.strip()  # Only strips leading/trailing spaces\n",
        "\n",
        "    # Map labels to integers based on actual label values\n",
        "    label_mapping = {\"ABUSIVE\": 1, \"NOT ABUSIVE\": 0}\n",
        "    data['label'] = data['level1'].map(label_mapping)\n",
        "\n",
        "    # Check if mapping worked (print a few rows)\n",
        "    print(data[['level1', 'label']].head())\n",
        "\n",
        "    # Structure data into input (text) and output (label)\n",
        "    input_text = data['text'].tolist()\n",
        "    output_labels = data['label'].tolist()\n",
        "\n",
        "    return input_text, output_labels, data\n",
        "\n",
        "# Load data\n",
        "input_text, output_labels, data = load_english_data(\"dataset-en.tsv\")\n",
        "\n",
        "# 80% train and dev, 20% test\n",
        "train_dev_texts, test_texts, train_dev_labels, test_labels = train_test_split(\n",
        "    input_text, output_labels, test_size=0.2, random_state=42, stratify=output_labels\n",
        ")\n",
        "\n",
        "# split 80 (train and dev) into 87.5% train and 12.5% dev (resulting in 70/10 split)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_dev_texts, train_dev_labels, test_size=0.125, random_state=42, stratify=train_dev_labels\n",
        ")\n",
        "\n",
        "# Print some samples\n",
        "print(\"Training Texts Sample:\", train_texts[:3])\n",
        "print(\"Training Labels Sample:\", train_labels[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u87GmEE3WL7s"
      },
      "source": [
        "Load and tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CP61tEeWKaS",
        "outputId": "654ec662-7f11-4fd4-d792-4de12be37e39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# load the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# pad and truncate\n",
        "def tokenize_data(texts, tokenizer, max_length=128):\n",
        "    encoded_texts = tokenizer(\n",
        "        texts,\n",
        "        padding=True,  # Pad the sequences to the same length\n",
        "        truncation=True,  # Truncate the sequences to max_length\n",
        "        max_length=max_length,  # Set max token length\n",
        "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
        "        return_attention_mask=True  # Include attention mask\n",
        "    )\n",
        "    return encoded_texts\n",
        "\n",
        "# Tokenize the training and validation data\n",
        "train_encodings = tokenize_data(train_texts, tokenizer)\n",
        "val_encodings = tokenize_data(val_texts, tokenizer)\n",
        "test_encodings = tokenize_data(test_texts, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWb-0G3oWR0G"
      },
      "outputs": [],
      "source": [
        "# Convert tokenized data and labels into Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': torch.tensor(train_labels)  # Ensure labels are tensors\n",
        "})\n",
        "\n",
        "# validation\n",
        "val_dataset = Dataset.from_dict({\n",
        "    'input_ids': val_encodings['input_ids'],\n",
        "    'attention_mask': val_encodings['attention_mask'],\n",
        "    'labels': torch.tensor(val_labels)  # Ensure labels are tensors\n",
        "})\n",
        "\n",
        "# testing\n",
        "test_dataset = Dataset.from_dict({\n",
        "    'input_ids': test_encodings['input_ids'],\n",
        "    'attention_mask': test_encodings['attention_mask'],\n",
        "    'labels': torch.tensor(test_labels)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpWpSkupWS8e",
        "outputId": "fb10bf32-53c5-4af7-ff1d-81ad036a1a6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",               # Directory where the model checkpoints will be saved\n",
        "    evaluation_strategy=\"steps\",          # Evaluate periodically during training\n",
        "    logging_steps=500,                    # Log every 500 steps\n",
        "    save_strategy=\"steps\",                # Save checkpoints periodically\n",
        "    save_steps=1000,                      # Save every 1000 steps\n",
        "    save_total_limit=1,                   # Keep only the latest checkpoint\n",
        "    per_device_train_batch_size=32,       # Larger batch size for faster training\n",
        "    per_device_eval_batch_size=16,        # Batch size for evaluation\n",
        "    num_train_epochs=3,                   # Number of training epochs\n",
        "    learning_rate=5e-5,                   # Learning rate\n",
        "    weight_decay=0.01,                    # Weight decay for regularization\n",
        "    load_best_model_at_end=True,          # Load the best model after training\n",
        "    metric_for_best_model=\"f1\",           # Use F1-score for model selection\n",
        "    fp16=True,                            # Enable mixed precision\n",
        "    dataloader_num_workers=4,             # Parallel data loading\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtX5vSNMWVrE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)  # Get predicted labels\n",
        "\n",
        "    # Debugging: Print predictions and labels\n",
        "    print(\"Sample Predictions:\", preds[:10])\n",
        "    print(\"Sample Labels:\", labels[:10])\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9NhZw3rUFk"
      },
      "source": [
        "## English Dataset Trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "1OfnqZvkWsgQ",
        "outputId": "1b0b7de6-9664-4fa1-e5bf-44b1c864f4d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-6-a69af0238e5c>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmargaretrose-bowen\u001b[0m (\u001b[33mmargaretrose-bowen-universit-di-trento\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250112_140112-0tapz9mm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/margaretrose-bowen-universit-di-trento/huggingface/runs/0tapz9mm' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/margaretrose-bowen-universit-di-trento/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/margaretrose-bowen-universit-di-trento/huggingface' target=\"_blank\">https://wandb.ai/margaretrose-bowen-universit-di-trento/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/margaretrose-bowen-universit-di-trento/huggingface/runs/0tapz9mm' target=\"_blank\">https://wandb.ai/margaretrose-bowen-universit-di-trento/huggingface/runs/0tapz9mm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='115' max='519' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [115/519 1:28:27 < 5:16:14, 0.02 it/s, Epoch 0.66/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the pre-trained multilingual BERT model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                          # The model to train\n",
        "    args=training_args,                   # Training arguments\n",
        "    train_dataset=train_dataset,          # The training dataset\n",
        "    eval_dataset=val_dataset,             # The validation dataset\n",
        "    tokenizer=tokenizer,                  # The tokenizer to process text during training\n",
        "    compute_metrics=compute_metrics       # Metrics to evaluate the model\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./trained_model\")  # Saves the trained model to a directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0d0wC31Wyo4"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"Evaluation English dataset Results:\", eval_results)\n",
        "\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Evaluation on English Test Set (20% holdout):\", test_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh4AlExHsdJW"
      },
      "source": [
        "## Italian Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57HsgXOTW5aY"
      },
      "outputs": [],
      "source": [
        "# check for nan values with math\n",
        "import math\n",
        "\n",
        "\n",
        "# Load Italian dataset\n",
        "def load_italian_data(filepath):\n",
        "   data = pd.read_csv(filepath, sep=\"\\t\")\n",
        "\n",
        "\n",
        "   # Clean and preprocess labels if they exist\n",
        "   data['level1'] = data['level1'].str.strip()  # Assuming the Italian dataset has a 'level1' column\n",
        "   # note: Italian dataset, they randomly don't have the whole word ABUSIVE, just ABUS\n",
        "   label_mapping = {\"ABUS\": 1, \"NOT-ABUSIVE\": 0}  # Assuming same labels\n",
        "   data['label'] = data['level1'].map(label_mapping)\n",
        "\n",
        "\n",
        "   # Check for NaN labels\n",
        "   if data['label'].isna().any():\n",
        "       print(\"Rows with NaN labels (Italian):\")\n",
        "       print(data[data['label'].isna()])\n",
        "\n",
        "\n",
        "   # Extract text and labels\n",
        "   input_text = data['text'].tolist()\n",
        "   output_labels = data['label'].tolist()\n",
        "\n",
        "\n",
        "   return input_text, output_labels\n",
        "\n",
        "\n",
        "# Load the Italian dataset\n",
        "italian_texts, italian_labels = load_italian_data(\"dataset-it.tsv\")\n",
        "\n",
        "\n",
        "# 80% train and dev, 20% test\n",
        "italian_train_dev_texts, italian_test_texts, italian_train_dev_labels, italian_test_labels = train_test_split(\n",
        "    italian_texts, italian_labels, test_size=0.2, random_state=42, stratify=italian_labels\n",
        ")\n",
        "\n",
        "# split 80% (train and dev) into 87.5% train and 12.5% val â†’ 70/10 split overall\n",
        "italian_train_texts, italian_val_texts, italian_train_labels, italian_val_labels = train_test_split(\n",
        "    italian_train_dev_texts, italian_train_dev_labels, test_size=0.125, random_state=42, stratify=italian_train_dev_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhV83L1xsk6K"
      },
      "outputs": [],
      "source": [
        "# Tokenize the training and validation datasets\n",
        "italian_train_encodings = tokenize_data(italian_train_texts, tokenizer)\n",
        "italian_val_encodings = tokenize_data(italian_val_texts, tokenizer)\n",
        "italian_test_encodings = tokenize_data(italian_test_texts, tokenizer)\n",
        "\n",
        "\n",
        "# Convert tokenized tensors to lists\n",
        "train_input_ids_list = italian_train_encodings['input_ids'].tolist()\n",
        "train_attention_mask_list = italian_train_encodings['attention_mask'].tolist()\n",
        "val_input_ids_list = italian_val_encodings['input_ids'].tolist()\n",
        "val_attention_mask_list = italian_val_encodings['attention_mask'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trxfWeK1snbd"
      },
      "outputs": [],
      "source": [
        "# Convert to Hugging Face Dataset objects\n",
        "italian_train_dataset = Dataset.from_dict({\n",
        "   'input_ids': train_input_ids_list,\n",
        "   'attention_mask': train_attention_mask_list,\n",
        "   'labels': torch.tensor(italian_train_labels)\n",
        "})\n",
        "\n",
        "\n",
        "italian_val_dataset = Dataset.from_dict({\n",
        "   'input_ids': val_input_ids_list,\n",
        "   'attention_mask': val_attention_mask_list,\n",
        "   'labels': torch.tensor(italian_val_labels)\n",
        "})\n",
        "\n",
        "# Build HuggingFace Dataset for the Italian test set\n",
        "italian_test_dataset = Dataset.from_dict({\n",
        "    'input_ids': italian_test_encodings['input_ids'],\n",
        "    'attention_mask': italian_test_encodings['attention_mask'],\n",
        "    'labels': torch.tensor(italian_test_labels)\n",
        "})\n",
        "\n",
        "# print each to see if its working alright\n",
        "print(\"Italian Training Dataset:\", italian_train_dataset)\n",
        "print(\"Italian Validation Dataset:\", italian_val_dataset)\n",
        "print(\"Italian Test Dataset:\", italian_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTQav07oswVb"
      },
      "outputs": [],
      "source": [
        "# error checking cuz i was having some issues previously\n",
        "# Check unique values in level1 column\n",
        "print(\"Unique values in level1 column:\", data['level1'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qLkfAGQs0qv"
      },
      "outputs": [],
      "source": [
        "# Italian Dataset\n",
        "italian_texts, italian_labels = load_italian_data(\"dataset-it.tsv\")\n",
        "print(\"Italian unique labels:\", set(italian_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_js3tIrs8OF"
      },
      "source": [
        "\n",
        "Trainer for evaluation. Use english model to evaluate on Italian validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOsXMpJWs7Tf"
      },
      "outputs": [],
      "source": [
        "# Load the trained English model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./trained_model\")\n",
        "\n",
        "\n",
        "# Initialize a Trainer for evaluation\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   tokenizer=tokenizer,\n",
        "   compute_metrics=compute_metrics  # Include the metrics function\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the English-trained model on the Italian validation dataset\n",
        "eval_results_italian = trainer.evaluate(italian_val_dataset)  # Use the validation split here\n",
        "\n",
        "italian_test_results = trainer.evaluate(italian_test_dataset)\n",
        "print(\"Evaluation on Italian Test Set (20% holdout):\", italian_test_results)\n",
        "\n",
        "# Print the zero-shot evaluation results\n",
        "print(\"Zero-shot evaluation results on the Italian validation dataset:\", eval_results_italian)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}